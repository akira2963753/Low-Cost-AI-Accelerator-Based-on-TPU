import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import numpy as np
import math

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'Using device: {device}')

np.random.seed(42)                  
torch.manual_seed(42) 

# Define the same LeNet model architecture as original
class LeNet(nn.Module):
    def __init__(self, num_classes=10):
        super(LeNet, self).__init__()
        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2)
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0)
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.fc1_input_size = 16 * 5 * 5
        self.fc1 = nn.Linear(self.fc1_input_size, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, num_classes)
        self.dropout = nn.Dropout(0.2)
        
    def forward(self, x):
        x = self.pool1(F.relu(self.conv1(x)))
        x = self.pool2(F.relu(self.conv2(x)))
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = self.dropout(x)
        x = self.fc3(x)
        return x

# æ”¹è¿›ç‰ˆé‡åŒ– LeNet - æ›´ä¿å®ˆçš„é‡åŒ–ç­–ç•¥
class ImprovedQuantizedLeNet(nn.Module):
    def __init__(self, num_classes=10, quantize_activations=True):
        super(ImprovedQuantizedLeNet, self).__init__()
        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2)
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0)
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.fc1_input_size = 16 * 5 * 5
        self.fc1 = nn.Linear(self.fc1_input_size, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, num_classes)
        self.dropout = nn.Dropout(0.2)
        self.quantize_activations = quantize_activations
        
    def forward(self, x):
        # å·ç§¯å±‚ï¼šåªåœ¨æ± åŒ–åé‡åŒ–ï¼Œå‡å°‘é‡åŒ–æ¬¡æ•°
        x = F.relu(self.conv1(x))
        x = self.pool1(x)
        if self.quantize_activations:
            x = improved_quantize_activation(x)
        
        x = F.relu(self.conv2(x))
        x = self.pool2(x)
        if self.quantize_activations:
            x = improved_quantize_activation(x)
        
        x = x.view(x.size(0), -1)
        
        # å…¨è¿æ¥å±‚ï¼šæ›´è°¨æ…çš„é‡åŒ–
        x = F.relu(self.fc1(x))
        if self.quantize_activations:
            x = improved_quantize_activation(x)
        x = self.dropout(x)
        
        x = F.relu(self.fc2(x))
        if self.quantize_activations:
            x = improved_quantize_activation(x)
        x = self.dropout(x)
        
        # æœ€åä¸€å±‚ä¸é‡åŒ–
        x = self.fc3(x)
        return x

# ==================== æ”¹è¿›çš„é‡åŒ–å‡½æ•° ====================

def analyze_tensor_range(tensor, tensor_name):
    """
    åˆ†æå¼ é‡çš„æ•°å€¼èŒƒå›´ï¼Œç”¨äºç¡®å®šæœ€ä½³é‡åŒ–å‚æ•°
    """
    tensor_np = tensor.detach().cpu().numpy().flatten()
    stats = {
        'name': tensor_name,
        'min': np.min(tensor_np),
        'max': np.max(tensor_np),
        'mean': np.mean(tensor_np),
        'std': np.std(tensor_np),
        'q1': np.percentile(tensor_np, 25),
        'median': np.percentile(tensor_np, 50),
        'q3': np.percentile(tensor_np, 75),
        'q95': np.percentile(tensor_np, 95),
        'q99': np.percentile(tensor_np, 99)
    }
    return stats

def determine_optimal_scale(tensor, target_bits=8):
    """
    æ ¹æ®å¼ é‡åˆ†å¸ƒç¡®å®šæœ€ä¼˜ç¼©æ”¾å› å­
    """
    tensor_np = tensor.detach().cpu().numpy().flatten()
    
    # ä½¿ç”¨ 95% åˆ†ä½æ•°æ¥ç¡®å®šèŒƒå›´ï¼Œé¿å…æå€¼å½±å“
    abs_max = max(abs(np.percentile(tensor_np, 2.5)), abs(np.percentile(tensor_np, 97.5)))
    
    # ä¸º8ä½æœ‰ç¬¦å·æ•´æ•°è®¡ç®—ç¼©æ”¾å› å­
    max_int = 2**(target_bits-1) - 1  # 127 for 8-bit
    
    if abs_max > 0:
        scale = max_int / abs_max
    else:
        scale = 128.0  # é»˜è®¤ Q1.7 ç¼©æ”¾
    
    return scale

def improved_float_to_fixed_point(weight, scale=128.0, bits=8):
    """
    æ”¹è¿›çš„æµ®ç‚¹åˆ°å®šç‚¹è½¬æ¢ï¼Œä½¿ç”¨åŠ¨æ€ç¼©æ”¾
    """
    # ä½¿ç”¨æä¾›çš„ç¼©æ”¾å› å­
    scaled_weight = weight * scale
    
    # èˆå…¥åˆ°æœ€è¿‘æ•´æ•°
    rounded_weight = math.floor(scaled_weight)
    
    # é™åˆ¶åˆ°æŒ‡å®šä½æ•°çš„æœ‰ç¬¦å·èŒƒå›´
    max_val = 2**(bits-1) - 1
    min_val = -2**(bits-1)
    clamped_weight = max(min_val, min(max_val, rounded_weight))
    
    # è½¬æ¢ä¸ºäºŒè¿›åˆ¶å­—ç¬¦ä¸²
    if clamped_weight >= 0:
        binary_str = format(clamped_weight, f'0{bits}b')
    else:
        unsigned_val = (1 << bits) + clamped_weight
        binary_str = format(unsigned_val, f'0{bits}b')
    
    return binary_str

def improved_binary_to_float(binary_str, scale=128.0, bits=8):
    """
    æ”¹è¿›çš„äºŒè¿›åˆ¶åˆ°æµ®ç‚¹è½¬æ¢
    """
    int_val = int(binary_str, 2)
    
    # å¤„ç†è´Ÿæ•°ï¼ˆäºŒè¿›åˆ¶è¡¥ç ï¼‰
    if int_val >= 2**(bits-1):
        int_val = int_val - 2**bits
    
    # è½¬æ¢å›æµ®ç‚¹æ•°
    float_val = int_val / scale
    return float_val

def conservative_msr4_compensation(binary_str):
    """
    ä¿å®ˆçš„ MSR-4 è¡¥å¿ç­–ç•¥
    """
    if has_msr4(binary_str):
        # MSR-4: åªè®¾ç½®æœ€åä¸€ä½ä¸º1
        compensated = binary_str[:-1] + '1'
    else:
        # Non-MSR-4: ä¿æŒåŸæ ·æˆ–è½»å¾®è°ƒæ•´
        #compensated = binary_str
        compensated = binary_str[:-1] + '1'
        #compensated = binary_str[:-4] + '1000'
    return compensated

def improved_quantize_activation(activation_tensor):
    """
    æ”¹è¿›çš„æ¿€æ´»å€¼é‡åŒ–ï¼Œä½¿ç”¨åŠ¨æ€ç¼©æ”¾å’Œæ›´ä¿å®ˆçš„è¡¥å¿
    """
    # åˆ†ææ¿€æ´»å€¼èŒƒå›´
    scale = determine_optimal_scale(activation_tensor)
    
    # è½¬æ¢ä¸ºnumpyå¤„ç†
    activations_np = activation_tensor.detach().cpu().numpy()
    original_shape = activations_np.shape
    activations_flat = activations_np.flatten()
    
    quantized_activations = []
    
    for activation in activations_flat:
        # ä½¿ç”¨åŠ¨æ€ç¼©æ”¾è¿›è¡Œé‡åŒ–
        binary_str = improved_float_to_fixed_point(activation, scale)
        
        # ä¿å®ˆçš„è¡¥å¿ç­–ç•¥
        compensated_binary = conservative_msr4_compensation(binary_str[:-4] + '1000')
        
        # è½¬æ¢å›æµ®ç‚¹
        quantized_float = improved_binary_to_float(compensated_binary, scale)
        quantized_activations.append(quantized_float)
    
    # è½¬æ¢å›å¼ é‡
    quantized_tensor = torch.tensor(quantized_activations, dtype=torch.float32).reshape(original_shape)
    return quantized_tensor.to(activation_tensor.device)

def improved_quantize_weight_tensor(weight_tensor, layer_name):
    """
    æ”¹è¿›çš„æƒé‡é‡åŒ–ï¼Œä½¿ç”¨åŠ¨æ€ç¼©æ”¾
    """
    # åˆ†ææƒé‡èŒƒå›´
    stats = analyze_tensor_range(weight_tensor, layer_name)
    print(f"  {layer_name} range: [{stats['min']:.6f}, {stats['max']:.6f}], "
          f"mean: {stats['mean']:.6f}, std: {stats['std']:.6f}")
    
    # ç¡®å®šæœ€ä¼˜ç¼©æ”¾å› å­
    scale = determine_optimal_scale(weight_tensor)
    print(f"  Using scale factor: {scale:.2f}")
    
    original_shape = weight_tensor.shape
    weights_flat = weight_tensor.flatten()
    
    quantized_weights = []
    msr4_count = 0
    non_msr4_count = 0
    
    for i, weight in enumerate(weights_flat):
        # ä½¿ç”¨åŠ¨æ€ç¼©æ”¾è¿›è¡Œé‡åŒ–
        binary_str = improved_float_to_fixed_point(weight.item(), scale)
        
        # æ£€æŸ¥ MSR-4
        if has_msr4(binary_str):
            msr4_count += 1
        else:
            non_msr4_count += 1
        
        # ä¿å®ˆçš„è¡¥å¿ç­–ç•¥
        compensated_binary = conservative_msr4_compensation(binary_str)
        
        # è½¬æ¢å›æµ®ç‚¹
        quantized_float = improved_binary_to_float(compensated_binary, scale)
        quantized_weights.append(quantized_float)
    
    # è½¬æ¢å›å¼ é‡
    quantized_tensor = torch.tensor(quantized_weights, dtype=torch.float32).reshape(original_shape)
    
    stats_dict = {
        'layer_name': layer_name,
        'total_weights': len(weights_flat),
        'msr4_count': msr4_count,
        'non_msr4_count': non_msr4_count,
        'msr4_percentage': (msr4_count / len(weights_flat)) * 100,
        'non_msr4_percentage': (non_msr4_count / len(weights_flat)) * 100,
        'scale_factor': scale,
        'original_range': f"[{stats['min']:.6f}, {stats['max']:.6f}]"
    }
    
    return quantized_tensor, stats_dict

# å¤ç”¨åŸæœ‰çš„è¾…åŠ©å‡½æ•°
def has_msr4(binary_str):
    """æ£€æŸ¥æ˜¯å¦æœ‰ MSR-4"""
    if len(binary_str) != 8:
        return False
    first_four = binary_str[:4]
    return first_four == '0000' or first_four == '1111'

# ==================== å¤šç§é‡åŒ–ç­–ç•¥ ====================

def create_quantized_model_strategy(original_model, strategy="conservative"):
    """
    åˆ›å»ºä¸åŒç­–ç•¥çš„é‡åŒ–æ¨¡å‹
    """
    strategies = {
        "weights_only": {"quantize_weights": True, "quantize_activations": False},
        "conservative": {"quantize_weights": True, "quantize_activations": True},
        "aggressive": {"quantize_weights": True, "quantize_activations": True}
    }
    
    if strategy not in strategies:
        strategy = "conservative"
    
    config = strategies[strategy]
    print(f"Creating quantized model with strategy: {strategy}")
    print(f"  - Quantize weights: {config['quantize_weights']}")
    print(f"  - Quantize activations: {config['quantize_activations']}")
    
    # åˆ›å»ºé‡åŒ–æ¨¡å‹
    quantized_model = ImprovedQuantizedLeNet(quantize_activations=config['quantize_activations']).to(device)
    
    # é‡åŒ–æƒé‡
    quantization_stats = []
    original_state_dict = original_model.state_dict()
    quantized_state_dict = {}
    
    for name, param in original_state_dict.items():
        if 'weight' in name and config['quantize_weights']:
            # é‡åŒ–æƒé‡
            quantized_tensor, stats = improved_quantize_weight_tensor(param, name)
            quantized_state_dict[name] = quantized_tensor
            quantization_stats.append(stats)
            print(f"  âœ“ {name}: MSR-4: {stats['msr4_count']:,} ({stats['msr4_percentage']:.2f}%), "
                  f"Scale: {stats['scale_factor']:.2f}")
        else:
            # ä¿æŒåŸå§‹å‚æ•°
            quantized_state_dict[name] = param
            param_type = "bias" if 'bias' in name else "weight (not quantized)"
            print(f"  âœ“ {name}: Kept original ({param_type})")
    
    # åŠ è½½é‡åŒ–æƒé‡
    quantized_model.load_state_dict(quantized_state_dict)
    
    return quantized_model, quantization_stats

# ==================== è¯„ä¼°å‡½æ•° ====================

def evaluate_model(model, test_loader, model_name="Model"):
    """è¯„ä¼°æ¨¡å‹å‡†ç¡®ç‡"""
    model.eval()
    correct = 0
    total = 0
    
    print(f"Evaluating {model_name}...")
    
    with torch.no_grad():
        for batch_idx, (data, target) in enumerate(test_loader):
            data, target = data.to(device), target.to(device)
            outputs = model(data)
            _, predicted = torch.max(outputs, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
            
            if (batch_idx + 1) % 20 == 0:
                current_acc = 100 * correct / total
                print(f"  Batch {batch_idx + 1}: Current accuracy: {current_acc:.2f}%")
    
    accuracy = 100 * correct / total
    print(f"  Final {model_name} Accuracy: {accuracy:.2f}%")
    return accuracy

def get_data_loaders(batch_size=64):
    """è·å–æ•°æ®åŠ è½½å™¨"""
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    
    test_dataset = datasets.MNIST('data', train=False, download=True, transform=transform)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    return test_loader

# ==================== ç»“æœåˆ†æ ====================

def display_improved_results(original_acc, strategies_results):
    """æ˜¾ç¤ºæ”¹è¿›çš„ç»“æœå¯¹æ¯”"""
    print("\n" + "="*90)
    print("IMPROVED LENET Q1.7 QUANTIZATION RESULTS")
    print("="*90)
    
    print(f"\nğŸ¯ ACCURACY COMPARISON:")
    print(f"{'Strategy':<20} {'Accuracy':>10} {'Drop':>10} {'Retention':>12} {'Description'}")
    print("-" * 80)
    
    print(f"{'Original':<20} {original_acc:>9.2f}% {0.0:>9.2f}% {100.0:>11.2f}% {'Float32 baseline'}")
    
    for strategy, (accuracy, stats) in strategies_results.items():
        drop = original_acc - accuracy
        retention = (accuracy / original_acc) * 100
        
        descriptions = {
            "weights_only": "Weights quantized only",
            "conservative": "Weights + careful activations",
            "aggressive": "Full quantization"
        }
        
        print(f"{strategy:<20} {accuracy:>9.2f}% {drop:>9.2f}% {retention:>11.2f}% {descriptions.get(strategy, '')}")
    
    # æ‰¾å‡ºæœ€ä½³ç­–ç•¥
    best_strategy = max(strategies_results.keys(), key=lambda x: strategies_results[x][0])
    best_acc = strategies_results[best_strategy][0]
    
    print(f"\nğŸ† BEST STRATEGY: {best_strategy} (Accuracy: {best_acc:.2f}%)")
    
    # æ˜¾ç¤ºé‡åŒ–ç»Ÿè®¡
    print(f"\nğŸ“Š QUANTIZATION STATISTICS:")
    for strategy, (accuracy, stats) in strategies_results.items():
        if stats:
            total_weights = sum(s['total_weights'] for s in stats)
            total_msr4 = sum(s['msr4_count'] for s in stats)
            msr4_percentage = (total_msr4 / total_weights) * 100 if total_weights > 0 else 0
            
            print(f"{strategy}: {total_weights:,} weights, {msr4_percentage:.2f}% MSR-4")
    
    print("="*90)

# ==================== ä¸»å‡½æ•° ====================

def main():
    """ä¸»å‡½æ•°"""
    print("Improved LeNet Q1.7 Quantization Analysis")
    print("="*50)
    
    # åŠ è½½åŸå§‹æ¨¡å‹
    print(f"\nğŸ“ Loading original trained LeNet model...")
    original_model = LeNet().to(device)
    
    try:
        state_dict = torch.load('mnist_lenet_model.pth', map_location=device)
        original_model.load_state_dict(state_dict)
        print("âœ“ Original LeNet model loaded successfully")
    except FileNotFoundError:
        print("âŒ Error: 'mnist_lenet_model.pth' not found!")
        print("Please train the model first using LeNet.py")
        return
    
    # è·å–æµ‹è¯•æ•°æ®
    print(f"\nğŸ“Š Loading test dataset...")
    test_loader = get_data_loaders(batch_size=1000)
    print("âœ“ Test dataset loaded")
    
    # è¯„ä¼°åŸå§‹æ¨¡å‹
    print(f"\nğŸ” Evaluating original Float32 LeNet model...")
    original_accuracy = evaluate_model(original_model, test_loader, "Original Float32 LeNet")
    
    # æµ‹è¯•å¤šç§é‡åŒ–ç­–ç•¥
    strategies = ["weights_only", "conservative", "aggressive"]
    strategies_results = {}
    
    for strategy in strategies:
        print(f"\nâš™ï¸  Testing strategy: {strategy}")
        quantized_model, stats = create_quantized_model_strategy(original_model, strategy)
        
        print(f"\nğŸ” Evaluating {strategy} quantized model...")
        accuracy = evaluate_model(quantized_model, test_loader, f"LeNet ({strategy})")
        
        strategies_results[strategy] = (accuracy, stats)
        
        # ä¿å­˜æ¨¡å‹
        torch.save(quantized_model.state_dict(), f'mnist_lenet_q17_{strategy}.pth')
        print(f"âœ“ Model saved as 'mnist_lenet_q17_{strategy}.pth'")
    
    # æ˜¾ç¤ºç»“æœ
    display_improved_results(original_accuracy, strategies_results)
    
    print(f"\nğŸ‰ Improved LeNet Q1.7 quantization analysis completed!")
    
    return original_model, strategies_results

if __name__ == "__main__":
    main()